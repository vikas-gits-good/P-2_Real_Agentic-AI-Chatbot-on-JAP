#!/usr/bin/env python3
"""
Generated by Crawl4AI Chrome Extension
URL: https://csjoseph.life/what-is-extraverted-thinking-te-cognitive-functions/
Generated: 2025-11-29T12:02:13.062Z
"""

import asyncio
import json
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

# The extraction schema generated from your selections
EXTRACTION_SCHEMA = {
    "name": "csjoseph.life Schema",
    "baseSelector": "#et-main-area > #main-content",
    "fields": [
        {
            "name": "video_name",
            "selector": "div.et_post_meta_wrapper > h1.entry-title",
            "type": "text",
        },
        {
            "name": "transcipt",
            "selector": "div.et_pb_section.et_pb_section_1 > div.et_pb_row.et_pb_row_1 > div.et_pb_column.et_pb_column_4_4",
            "type": "text",
        },
    ],
}


async def extract_data(
    url: str = "https://csjoseph.life/what-is-extraverted-thinking-te-cognitive-functions/",
):
    """Extract data using the generated schema"""

    # Configure browser (optional)
    browser_config = BrowserConfig(
        headless=True,  # Set to False to see the browser
        verbose=False,
    )

    # Configure extraction strategy
    extraction_strategy = JsonCssExtractionStrategy(schema=EXTRACTION_SCHEMA)

    # Configure crawler
    crawler_config = CrawlerRunConfig(
        extraction_strategy=extraction_strategy,
        # Add more options as needed:
        # wait_for="css:.product",  # Wait for specific elements
        # js_code="window.scrollTo(0, document.body.scrollHeight);",  # Execute JS
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url=url, config=crawler_config)

        if result.success and result.extracted_content:
            data = json.loads(result.extracted_content)
            print(f"\n‚úÖ Successfully extracted {len(data)} items!")

            # Save results
            with open("extracted_data.json", "w", encoding="utf-8") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

            # Show sample results
            print("\nüìä Sample results (first 2 items):")
            for i, item in enumerate(data[:2], 1):
                print(f"\nItem {i}:")
                for key, value in item.items():
                    print(f"  {key}: {value}")

            return data
        else:
            print("‚ùå Extraction failed:", result.error_message)
            return None


if __name__ == "__main__":
    # Run the extraction
    data = asyncio.run(extract_data())

    print("\nüéØ Next steps:")
    print("1. Install Crawl4AI: pip install crawl4ai")
    print("2. Modify the URL or add multiple URLs")
    print("3. Customize crawler options as needed")
    print("4. Check 'extracted_data.json' for full results")
